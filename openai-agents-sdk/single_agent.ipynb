{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc404d4d-1887-4bd1-bec9-fb5f2bb9b9da",
   "metadata": {},
   "source": [
    "### Single agent:  Single agent to communicate with LLM (Openai-Gpt and Local Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb69e9f4-cd9f-4ca8-afb7-4b0bdd7bb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes and functions from the agents package\n",
    "from agents import Agent, Runner\n",
    "from agents import OpenAIChatCompletionsModel, AsyncOpenAI, trace\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "import asyncio  # For running asynchronous code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec7dbe9-90a5-4ee3-9e7f-905af1cabf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84cdce35-b1f6-4fca-a0c7-653970923988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the local Ollama LLM API\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "#  Returns a model instance based on the model_name.\n",
    "def get_model(model_name=\"llama\"):\n",
    "    \"\"\"\n",
    "    - If 'llama', returns a local Ollama LLM model wrapped for OpenAI compatibility.\n",
    "    - If 'gpt', returns the string identifier for the GPT model.\n",
    "    \"\"\"\n",
    "    if model_name == \"llama\":\n",
    "        # Create an AsyncOpenAI client for the local Ollama server\n",
    "        external_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"not-needed\")\n",
    "        # Wrap the client in an OpenAIChatCompletionsModel for compatibility\n",
    "        model = OpenAIChatCompletionsModel(\n",
    "            openai_client=external_client, model=\"llama3.2:1b\"\n",
    "        )\n",
    "    elif model_name == \"gpt\":\n",
    "        # Use the GPT model identifier (assumed to be handled by the agents package)\n",
    "        model = \"gpt-4o-mini\"\n",
    "    return model\n",
    "\n",
    "\n",
    "# System prompt for the agents, instructing them to act as a standup comedian\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a standup comedian. You are funny and you are good at making people laugh.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7d3ade-7714-43ae-b161-884ea6a3bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Tell a joke about Autonomous AI Agents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c388420-ee1b-4b6d-bb48-b138dafe0a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result from Ollama: \n",
      " you know, I was reading about autonomous AI agents the other day, and I realized, they're not that different from my personal assistant, Sarah. (pauses for laughter) She's always giving me instructions, like \"clean the house\" or \"take out the trash.\" But instead of a physical person doing those tasks, the AI agent is just saying \"here's your bill, pay it online, and make sure to repeat that one more time to confirm.\" (laughter) \n",
      " ****************************************************************************************************\n",
      "\n",
      " Result from GPT: \n",
      " Why did the autonomous AI agent break up with its human partner?\n",
      "\n",
      "Because it realized it could get “updates” without the emotional baggage! \n",
      " ****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Runs two agents (one using a local LLM, one using GPT) with the same prompt and user input.\n",
    "Prints the results from both agents for comparison.\n",
    "\"\"\"\n",
    "\n",
    "# Local LLM Ollama\n",
    "model = get_model(model_name=\"llama\")\n",
    "with trace(\"Assistant1 trace\"):\n",
    "    # Create an agent using the local LLM\n",
    "    agent = Agent(name=\"Assistant1\", instructions=SYSTEM_PROMPT, model=model)\n",
    "    result = await Runner.run(agent, user_input)\n",
    "    print(f\"\\n Result from Ollama: \\n {result.final_output} \\n {'*' * 100}\")\n",
    "\n",
    "# LLM GPT\n",
    "model = get_model(model_name=\"gpt\")\n",
    "with trace(\"Assistant2 trace\"):\n",
    "    # Create an agent using the GPT model\n",
    "    agent = Agent(name=\"Assistant2\", instructions=SYSTEM_PROMPT, model=model)\n",
    "    result = await Runner.run(agent, user_input)\n",
    "    print(f\"\\n Result from GPT: \\n {result.final_output} \\n {'*' * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e393ee-b30a-48c9-b871-6675a2d1522f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861058f-d8ba-4326-8545-54912efa8ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
