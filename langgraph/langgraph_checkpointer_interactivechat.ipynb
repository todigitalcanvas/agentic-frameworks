{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa726a45-d3be-4fd6-964e-26a119453266",
   "metadata": {},
   "source": [
    "#### LangGraph workflow with persistent memory integration, conditional edge. The agent can use tools (e.g., web search) and maintain chat history across sessions using SQLite or in-memory storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e378d49-947e-4953-91f0-fc7620ce7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import Tool\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Persistent memory imports\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8c427c-b40b-4067-be9d-5608e51ee950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script demonstrates a LangGraph workflow with persistent memory integration.\n",
    "# The agent can use tools (e.g., web search) and maintain chat history across sessions using SQLite or in-memory storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3ff1e7-6e1f-4214-af0a-dcf7c6cb0511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a432e0f-21b0-4e3d-a1c8-6c6a568c80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tools\n",
    "# Instantiate a Google Serper API wrapper for web search\n",
    "serper = GoogleSerperAPIWrapper()\n",
    "# Define a LangChain Tool for online search\n",
    "tool_search = Tool(\n",
    "        name=\"search\",\n",
    "        func=serper.run,\n",
    "        description=\"Useful for when you need more information from an online search\"\n",
    "    )\n",
    "\n",
    "# List of available tools for the agent\n",
    "tools = [tool_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4df698a-a8ef-4b5b-b509-3448fdb45a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to get an LLM instance by model name\n",
    "def get_llm(model_name: str):\n",
    "    if model_name == \"gpt\":\n",
    "        return ChatOpenAI(model=\"gpt-4o-mini\")  # Use OpenAI GPT model\n",
    "    elif model_name == \"llama\":\n",
    "        return ChatOllama(model=\"llama3.2:1b\")  # Use local Ollama Llama model\n",
    "\n",
    "# Helper function to get a memory saver (in-memory or SQLite for persistence)\n",
    "def get_memory(memory_type: str = \"in-memory\"):\n",
    "    if memory_type == \"in-memory\":\n",
    "        return MemorySaver()  # Volatile, in-memory storage\n",
    "    elif memory_type == \"sqlite\":\n",
    "        db_path = \"memory.db\"\n",
    "        conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "        sql_memory = SqliteSaver(conn)  # Persistent SQLite storage\n",
    "        return sql_memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874eb34f-eca9-4ee9-aee2-ecbb4c16ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1. Define State\n",
    "class State(BaseModel):\n",
    "    # The state holds a list of messages. The add_messages reducer will append new messages to this list.\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6732cd-b6b7-4a92-9bb6-15fd11f40142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your message:  I am John, introduce your self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello John! I'm an AI language model designed to assist you with a variety of questions and tasks. Whether you need information, help with writing, or just want to engage in conversation, I'm here to help. What can I do for you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your message:  What is capital of France\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The capital of France is Paris.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your message:  What is my name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Your name is John.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup LLM and bind tools to it\n",
    "llm = get_llm(\"gpt\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Setup memory - save chat history (choose between in-memory or SQLite)\n",
    "memory = get_memory(\"sqlite\")\n",
    "\n",
    "## Step 2 -> start graph builder\n",
    "graph_builder = StateGraph(State)  # Initialize a stateful graph with the State schema\n",
    "\n",
    "## Step 3 -> Define Nodes\n",
    "# The chatbot node: generates a response using the LLM (with tool access)\n",
    "def chatbot(state: State) -> State:\n",
    "    new_messages = [llm_with_tools.invoke(state.messages)]\n",
    "    return State(messages=new_messages)\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)  # Main conversational node\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))  # Tool node for executing tool calls\n",
    "\n",
    "\n",
    "## Step 4 -> create edge\n",
    "# Conditional edge: if the chatbot decides a tool is needed, transition to the tool node\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition, \"tools\")\n",
    "# After tool execution, return to the chatbot node\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "# Start the conversation at the chatbot node\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "# Allow the chatbot to end the conversation\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "\n",
    "## Step 5 -> compile graph with memory\n",
    "graph = graph_builder.compile(checkpointer=memory)  # Compile the graph with persistent memory\n",
    "\n",
    "## Step 6 -> create entry for chat and invoke graph\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}  # Use a thread ID for persistent chat sessions\n",
    "def chat(user_input: str, history):\n",
    "    # Helper function for single-turn chat (not used in main loop)\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "## Step 7 -> Accept user input and invoke graph\n",
    "# Interactive chat loop: accept user input, run the graph, and print the AI's response\n",
    "while True:\n",
    "    user_input = input(\"Enter your message: \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    print(f\"AI: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e878f-7f41-4253-8a3c-9a0a4e5f3cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
