{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5666b1f0-5301-48b6-8b47-4d115ac6b9fd",
   "metadata": {},
   "source": [
    "#### Intro to message and agent classes, register the agent, and send/receive messages asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f55e3f6-6158-42ac-bfb0-0647d8bb82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler, SingleThreadedAgentRuntime\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37f68501-fc11-4ae6-b887-68c73d7ce80a",
   "metadata": {},
   "source": [
    "# This script demonstrates an AutoGen agent that delegates joke generation to an LLM-powered assistant agent.\n",
    "# It shows how to set up model clients, delegate message handling, and orchestrate agent communication asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283a7838-242c-4a64-b89e-977a8ea8485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbfdf68-0b03-42c2-8c9e-dc41c8bc60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get a model client for the assistant agent\n",
    "def get_model_client(model_name: str):\n",
    "    if model_name == \"gpt\":\n",
    "        return OpenAIChatCompletionClient(model=\"gpt-4o-mini\")  # Use OpenAI GPT model\n",
    "    elif model_name == \"llama\":\n",
    "        return OllamaChatCompletionClient(model=\"llama3.2:1b\")  # Use local Ollama Llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476fee5e-8bde-4dd7-9d34-32b9dddb060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the message class\n",
    "# This class represents the structure of messages exchanged between agents and users\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Step 2: Define the agent class\n",
    "# This agent delegates joke generation to an LLM-powered AssistantAgent\n",
    "class JokeSterAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"JokeSter\")  # Initialize the agent with a name\n",
    "        model_client = get_model_client(\"gpt\")  # Choose the model client (GPT or Llama)\n",
    "        self._delegate = AssistantAgent(\"JokeSter\", model_client=model_client)  # Delegate for LLM responses\n",
    "\n",
    "    @message_handler\n",
    "    async def on_my_message(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        # This handler is called when the agent receives a message\n",
    "        user_message = message.content\n",
    "        \n",
    "        # Wrap the user message as a TextMessage for the assistant\n",
    "        text_message = TextMessage(content=user_message, source=\"user\")\n",
    "        # Delegate the message to the AssistantAgent and await the LLM's response\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        response_content = response.chat_message.content\n",
    "        \n",
    "        # Compose the reply, echoing the user's message and including the LLM-generated joke\n",
    "        reply = f\"\"\"This is {self.id.type}-{self.id.key}.\n",
    "        You said: '{user_message}'\n",
    "        Here's a joke for you: {response_content}\"\"\"\n",
    "        return Message(content=reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebb2940-e848-4fbc-b739-848ada6d7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Response: \n",
      " This is joke_ster_agent-default.\n",
      "        You said: 'Well hi there! Can you tell me a AI joke?'\n",
      "        Here's a joke for you: Sure! Here's a classic AI joke for you:\n",
      "\n",
      "Why did the robot go on a diet?\n",
      "\n",
      "Because it had too many bytes! \n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "## send message and run the agent\n",
    "# Step 3: Create the runtime (single-threaded for simplicity)\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "# Register the JokeStarAgent with the runtime\n",
    "await JokeSterAgent.register(runtime, \"joke_ster_agent\", lambda: JokeSterAgent())\n",
    "runtime.start()\n",
    "# Step 4: Create an AgentId for the registered agent\n",
    "agent_id = AgentId(\"joke_ster_agent\", \"default\")\n",
    "\n",
    "# Send a message to the agent and await the response\n",
    "result = await runtime.send_message(\n",
    "    Message(\"Well hi there! Can you tell me a AI joke?\"), agent_id\n",
    "    )\n",
    "\n",
    "print(f\"\\n Response: \\n {result.content}\")  # Print the agent's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77dfa55-b96a-42ae-89db-d1d60fe6b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
